Tasks to do:
=========================================================
PREPROCESSING STAGE
---------------------------------------------
1. Classify all tracks into folders
2. Cut all songs in each category into equal segments - so that mfcc are of equal sizes/matrices.
	Idea: We are essentially bothered about the cloud of song-sements belonging to each genre.
Its possible that each segments of different songs are more similar to other segments of another song, than other segments of itself. 
One challenge would be in determining the best time-interval of analysis for the songs. 

Find the shortest-duration of all songs in the list. Then divide the shortest song into 6 equal segments. 
The length of each segment becomes the interval into which all the songs shall be divided. 

3. Derive the mpcc and the dct of each song cloud point
4. Store/archive the sound-cloud-points in a meaningful manner. 
	+ Ensure that the mpcc,dct for the entire song is also stored. ( represent a song as a unique point.)
	+ Ensure that mpcc, dct for the song-intervals is stored (represent each song by a cloud.)

5. Determine the intrinsic dimensionality of these datasets by using correlational/dimension etc.

=========================================================

DIMENSION REDUCTION: 
---------------------------------------------
a) Linear: 
	1. Fast Johnson Lindenstrauss Transform Algorithm
	2. 

b) NonLinear:

=========================================================
CLUSTERING:

a) k-means

b) other-algo's

=========================================================

MEMBERSHIP COMPUTATION:

=========================================================

01-November:

# Use python for the tasks below: <Implement this on test-case of points on  a sphere?>

1. Create a test-case of data: Intersecting Spherical Data-Clusters
2. Generate Refined Graph-Embedding - using method described in class
3. Obtain Reduced Dimension representation - 3D - ScatterPlot

4. Implement FastJLTransform - Compare the Data-Dim_Reduction thus obtained. 

5. Spectral Clustering: Refined Embedding - implemented! 
	+ Then clustering can be done by using KNN Search.

6. Implement the entire process-flow using Matlab and Python. 

Important note: The algorithms take a lot of time to implement. Need to determine ways of implementing Laplacian and other operations in a distributed manner. 
Use Apache Spark. 

==================================================================================================================
==================================================================================================================

November 12:

Why use mfcc as a feature vector? 

Use of mfcc for modelling speech:
	represents speech amplitude spectrum
	process of creating mfcc:
		windows of 20ms frames that are statistically stationary : Hamming window removes edge effects.
		cepstral feature vector for each frame
			DFT -> log(spectrum_amplitude) : reject/discard phase information - not perceived by ears?
			smooth the spectrum - frequency bins: the 256 freqs are binned into 40 or so bins. 
			bin-spacing follows the mel-frequency scale. why? 
				lower freqs are perceptually more important than high frequency. 
			
			The components of the mel-spectral vectors calculated for each frame are highly correlated. 
			
			speech features are typically modeled by mixtures of Gaussian densities. 
			to reduce number of features in the system, the last step of the mfcc feature construction:
				apply a transform to the Mel-spectral vectors which decorrelates their components. 
				PCA (Karhunen-Loeve transform) achieves this : DCT
				Doing so, generates 13 or so cepstral features for each frame. 
				
MFCC for MUSIC ANALYSIS:

Stages of MFCC generation:
1. Divide signal into frames			: statistically stationary 
2. Extract spectra-amplitudes		: frequency domain representation
3. Take logarithms of the spectra-amplitudes: Perception
4. Convert to Mel-spectral			: Smoothing of amplitude spectra- by binning

It is sufficient to use MFCC for Music Analysis!
from: Mel Frequency Cepstral Coefficients for Music Modeling, Beth Logan, Cambridge Research Laboratory

==================================================================================================================
Music Retrieval Based on Melodic Similarity ~ Rainer Typke

Melodic Motion: characterized by successive pitch intervals
Rhythmic Aspect: Patterns are perceived wrt an underlying pulse that defines the tempo.

Melodic Search Engine: Distance measures are invariant under transportation, augmentation, or dimunition.
Not affected by: Tempo changes, Timbre changes. 

Pitch | Chroma | Timbre | Intervals | 

Features of melodies: can be used to calculate Markov Chains
1. Interval sequences
2. Pitch sequences
3. Rhythm 


Metrics to be used: -- as alternatives to Euclidean; Code it and then later use Metric Learning algorithm to contrast.
1. Earth Mover Distance
2. Proportional Transportation Distance


==================================================================================================================
Elias Pampalk - Thesis

Reconstructed Mel_Freq power specturm: M_dB_rec = DCT' * mfcc;
the number of Mel filters is 36, FFT step size is 512 and hop size is 512 (no overlap, although a Hann window is used).

Spectral similarity:
	Obtained by summarizing the mfcc information of frames: Clustering the frames!
	Distance between two pieces of music is computed by comparing their cluster models. 
	

3approaches to compute spectral similarity:
	1. Frame Clustering:
		GMM + Expectation Maximization - Netlab
		19-dimensional MFCC frame
		19x19 : covariance matrix
		M-gaussians
		Optimal Estimate THETA maximizes likelihood that frames are generated by the GMM.
			Measure: Log-likelihood
			
		Once each SONG has been summarized by a GMM -> 
		Then when comparing two songs, a SAMPLES from each GMM is sampled;
			
		Treat Each frame of the mfcc of each song as a data-point. 
		Learn a METRIC for all the frames from different songs. <This was my error>
		Create a GMM-30 to aggregate the information regarding the mfcc datasets.<Use Metric to compute 30 clusters>
		Develop the GMM model. 
		Sample the GMMs for each type to extract 2000 samples
		

		SingleGaussian Representation of each song.
			m = mean(mfcc,2); co = cov(mfcc'); ico = inv(co);
			symm_KL_div = trace(co1*ico2) + trace(co2*ico1) + trace((ico1+ico2)*(m1-m2)*(m1-m2)');
			dist-rescaling	: (used when combining the spectral distance with other information)
				d = -exp(-1/fact*d) ; fact = 450
		


==================================================================================================================
Fluctuation Patterns:
---------------------------

characteristics of audio signal not explained by spectral similarity.
a)SONOGRAM or a MEL-FREQUENCY-DB-spectrogram.
FP's based on MFCC:
	1. Cut spectrogram into short segments ( 6 seconds)
	2. Each segment and each frequency band, use FFT to compute the amplitude modulation frequencies of the loudness in the range 0-10Hz
	3. Weight the modulation frequencies using a model fo perceived fluctuation strengths.
	4. Some filters are applied to emphasize certain types of patterns. 

FP: matrix with rows corresponding to frequency bands  and columns = modulation frequencies (0-10Hz)

Summarizing FPs: Compute the MEDIAN of all FPs of different segments of the songs.
ONE FP MATRIX - represents an entire SONG.

Distance between the PIECES is computed by interpreting the FP-MATRIX as a high-dimensional vector. 

==================================================================================================================
How to combine the SPECTRAL SIMILARITY with the FLUCTUATIONS PATTERN?

	Multi-modal similarity - paper!

==================================================================================================================

Refer: Elias Pampalk Thesis.
-----------------------------------
Can we represent each song using a GMM?
If so, then how do we determine the distance between two GMMs( or two songs)?
Further, Can we do a Metric learning algorithm to find the distance between two GMMs?

Also, can we build up the Graph for Refined Embedding based on this GMM summarization of a song?

1.Train a GMM for each of the songs(mixtures of upto 50)
2.compute a similarity matrix between all songs using the likelihood of a song given a GMM. 
3.Based on the Genre Information, do nearest niehgbor classificationusing the similarity matrix. 

visualize the mfcc: M_db_rec = DCT'*mfcc


==================================================================================================================

Classifiers: 
1. GMM
2. SVM
3. SimpleGaussian
4. KNN 

==================================================================================================================
Arthur Flexer - Statistical Evaluation of Music Information Retrieval Systems

